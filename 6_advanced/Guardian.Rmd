---
output:
  md_document:
    variant: markdown_github
---

# Analyzing Meida Text

### Kohei Watanabe
### 24 April 2017

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE
)
```

### Overview

This section explains specific techniques useful in analyzing news articles:

* Constructing subject specific `tokens` object using `kwic`
* Identifying and concatenating multi-word expressions using `textstat_collocations()`

### Characteristics of news articles/transcripts

It is common to construct a corpus of news articles (or transcripts) from databses with keyword queries, but such a corpus still contains a lot of noises, because a news article is a set sentences and paragraphs on various subjects. To focus our analysis on particular subjects, we can construct a `tokens` by `kwic()`.

1. Construct a `tokens` as usual from a corpus
2. Run `kwic()` on the `tokens` to create a `kwic` 
3. Convert `kiwc` into `tokens` using `as.tokens`
4. Apply whatever analysis we want on `tokens`

### Constructing tokens from kwic

```{r}
require(quanteda)
load('guardianSample.RData')
toks <- tokens(guardianSample, remove_punct = TRUE)
toks <- tokens_remove(toks, stopwords('english'))

europe <- kwic(toks, c('Europe*', 'European Union', 'EU'))
head(europe)

britain <- kwic(toks, c('Brit*', 'United Kingdom', 'UK'))
head(britain)

toks_europe <- as.tokens(europe)
toks_britain <- as.tokens(britain)

```

### Run relative frequency analysis
```{r}
dfm_europe <- dfm(toks_europe)
dfm_britain <- dfm(toks_britain)
kwds <- textstat_keyness(rbind(dfm_europe, dfm_britain), target = seq_along(toks_europe))
head(kwds, 20)
tail(kwds, 20)

```

### Run dictionary analysis 
```{r}
dict <- dictionary(people = c('people', 'citizen*'), 
                   migrant = c('immigra*', 'migra*'),
                   economy = c('econom*', 'financ*', 'business*'),
                   crime = c('crim*', 'polic*', 'terror*'))

dfm_dict_europe <- dfm(toks_europe, dictionary = dict)
dfm_dict_britain <- dfm(toks_britain, dictionary = dict)

freq_dict_europe <- colSums(dfm_dict_europe) / sum(ntoken(toks_europe))
freq_dict_britain <- colSums(dfm_dict_britain) / sum(ntoken(toks_britain))

```
```{r, fig.width=5, fig.height=4}
plot(freq_dict_europe, ylab = 'Frequency', xlab = 'Category', xaxt = 'n', 
     ylim = c(0, 0.01), xlim = c(0.5, 4.5), main = "Frequency of keywords")
axis(1, at = 1:4, labels = names(freq_dict_europe))
grid()
points(freq_dict_britain, col = 'red', pch = 2)
legend('topright', legend = c('Europe', 'Britain'), col = c('black', 'red'), pch = 1:2)

```
### Identifying and concatenating multi-word expressions

```{r}
require(quanteda)
load('guardianSample.RData')
toks <- tokens(guardianSample, remove_punct = FALSE)
toks <- tokens_remove(toks, stopwords('english'), padding = TRUE)
col <- textstat_collocations(toks, method = 'bj', max_size = 5,
                             features = "^[A-Z][A-Za-z0-9]+$", valuetype = "regex", case_insensitive = FALSE)
head(col)
toks_multi <- tokens_compound(toks, col)
head(toks[['text166563']], 50)
head(toks_multi[['text166563']], 50)

```
