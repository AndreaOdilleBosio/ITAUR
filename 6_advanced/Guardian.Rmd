---
output:
  md_document:
    variant: gfm
---

# Analyzing Media Text

### Kohei Watanabe
### 28 March 2018 (updated)

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE
)
```

### Overview

This section explains specific techniques useful in analyzing news articles:

* Constructing subject specific `tokens` object using `kwic`
* Identifying and concatenating multi-word expressions using `textstat_collocations()`

### Characteristics of news articles/transcripts

It is common to construct a corpus of news articles (or transcripts) from databases with keyword queries, but such a corpus still contains a lot of noises, because a news article is a set sentences and paragraphs on various subjects. To focus our analysis on particular subjects, we can construct a `tokens` by `kwic()`.

1. Construct a `tokens` as usual from a corpus
2. Run `kwic()` on the `tokens` to create a `kwic` 
3. Convert `kiwc` into `tokens` using `as.tokens`
4. Apply whatever analysis we want on `tokens`

### Constructing tokens from kwic

```{r}
require(quanteda)
load('guardianSample.RData')
toks <- tokens(guardianSample, remove_punct = TRUE)
toks <- tokens_remove(toks, stopwords('english'))

europe <- kwic(toks, c('Europe*', 'European Union', 'EU'))
head(europe)

britain <- kwic(toks, c('Brit*', 'United Kingdom', 'UK'))
head(britain)
```

### Relative frequency analysis
```{r}
dfmcompare <- dfm(c(europe = europe$keyword, britain = britain$keyword))
kwds <- textstat_keyness(dfmcompare)
head(kwds, 20)
tail(kwds, 20)
```

### Targetted dictionary analysis 
```{r}
dict <- dictionary(list(people = c('people', 'citizen*'), 
                        migrant = c('immigra*', 'migra*'),
                        economy = c('econom*', 'financ*', 'business*'),
                        crime = c('crim*', 'polic*', 'terror*')))

dfm_dict_europe <- dfm(europe$keyword, dictionary = dict)
dfm_dict_britain <- dfm(britain$keyword, dictionary = dict)

freq_dict_europe <- colSums(dfm_dict_europe) / sum(ntoken(dfm_dict_europe))
freq_dict_britain <- colSums(dfm_dict_britain) / sum(ntoken(dfm_dict_europe))


plot(freq_dict_europe, ylab = 'Frequency', xlab = 'Category', xaxt = 'n', 
     ylim = c(0, 0.01), xlim = c(0.5, 4.5), main = "Frequency of keywords")
axis(1, at = 1:4, labels = names(freq_dict_europe))
grid()
points(freq_dict_britain, col = 'red', pch = 2)
legend('topright', legend = c('Europe', 'Britain'), col = c('black', 'red'), pch = 1:2)

```

### Identifying and concatenating multi-word expressions

```{r}
require(quanteda)
load('guardianSample.RData')
toks <- tokens(guardianSample, remove_punct = FALSE)
toks <- tokens_remove(toks, stopwords('english'), padding = TRUE)
col <- textstat_collocations(toks, method = 'bj', max_size = 5,
                             features = "^[A-Z][A-Za-z0-9]+$", valuetype = "regex", case_insensitive = FALSE)
head(col)
toks_multi <- tokens_compound(toks, col)
head(toks[['text166563']], 50)
head(toks_multi[['text166563']], 50)
```
